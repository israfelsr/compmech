{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "analysis-title",
   "metadata": {},
   "source": [
    "# Prompting Results Analysis\n",
    "\n",
    "This notebook analyzes the results from attribute prompting experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Load all JSON files from a folder\n",
    "results_folder = \"path/to/your/results/folder\"  # Update this path\n",
    "\n",
    "# Get all JSON files in the folder\n",
    "json_files = glob.glob(os.path.join(results_folder, \"*.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "# Load and combine all data\n",
    "all_data = {}\n",
    "for file_path in json_files:\n",
    "    print(f\"Loading {os.path.basename(file_path)}...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_data = json.load(f)\n",
    "        # Add filename info to each record for tracking\n",
    "        for record in file_data:\n",
    "            record['source_file'] = os.path.basename(file_path)\n",
    "        all_data.extend(file_data)\n",
    "\n",
    "print(f\"\\nTotal loaded examples: {len(all_data)}\")\n",
    "data = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert-responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert responses to numeric format for evaluation\n",
    "def response_to_numeric(response):\n",
    "    \"\"\"Convert 'True'/'False' responses to 1/0\"\"\"\n",
    "    if response.strip().lower() == 'true':\n",
    "        return 1\n",
    "    elif response.strip().lower() == 'false':\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"Unexpected response: {response}\")\n",
    "        return -1\n",
    "\n",
    "df['predicted'] = df['response'].apply(response_to_numeric)\n",
    "\n",
    "# Check for unexpected responses\n",
    "unexpected_responses = df[df['predicted'] == -1]\n",
    "if len(unexpected_responses) > 0:\n",
    "    print(f\"Found {len(unexpected_responses)} unexpected responses\")\n",
    "    print(unexpected_responses['response'].unique())\n",
    "else:\n",
    "    print(\"All responses are 'True' or 'False'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with unexpected responses\n",
    "df_clean = df[df['predicted'] != -1].copy()\n",
    "print(f\"Clean dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Extract true labels and predictions\n",
    "y_true = df_clean['label'].values\n",
    "y_pred = df_clean['predicted'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['False', 'True']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted False', 'Predicted True'],\n",
    "            yticklabels=['Actual False', 'Actual True'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distribution-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of responses\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# True labels distribution\n",
    "df_clean['label'].value_counts().plot(kind='bar', ax=ax1, color=['red', 'green'])\n",
    "ax1.set_title('Distribution of True Labels')\n",
    "ax1.set_xlabel('Label')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xticklabels(['False (0)', 'True (1)'], rotation=0)\n",
    "\n",
    "# Predicted labels distribution\n",
    "df_clean['predicted'].value_counts().plot(kind='bar', ax=ax2, color=['red', 'green'])\n",
    "ax2.set_title('Distribution of Predictions')\n",
    "ax2.set_xlabel('Prediction')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xticklabels(['False (0)', 'True (1)'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attribute-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis by attribute\n",
    "print(\"Performance by attribute:\")\n",
    "attribute_results = []\n",
    "\n",
    "for attr in df_clean['attribute'].unique():\n",
    "    attr_data = df_clean[df_clean['attribute'] == attr]\n",
    "    attr_accuracy = accuracy_score(attr_data['label'], attr_data['predicted'])\n",
    "    attr_f1 = f1_score(attr_data['label'], attr_data['predicted'])\n",
    "    \n",
    "    attribute_results.append({\n",
    "        'attribute': attr,\n",
    "        'count': len(attr_data),\n",
    "        'accuracy': attr_accuracy,\n",
    "        'f1_score': attr_f1\n",
    "    })\n",
    "    \n",
    "    print(f\"{attr}: {len(attr_data)} examples, Accuracy: {attr_accuracy:.4f}, F1: {attr_f1:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "attr_df = pd.DataFrame(attribute_results)\n",
    "attr_df = attr_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Analysis by source file (if applicable)\n",
    "if 'source_file' in df_clean.columns:\n",
    "    print(\"\\n\\nPerformance by source file:\")\n",
    "    file_results = []\n",
    "    \n",
    "    for file_name in df_clean['source_file'].unique():\n",
    "        file_data = df_clean[df_clean['source_file'] == file_name]\n",
    "        file_accuracy = accuracy_score(file_data['label'], file_data['predicted'])\n",
    "        file_f1 = f1_score(file_data['label'], file_data['predicted'])\n",
    "        \n",
    "        file_results.append({\n",
    "            'source_file': file_name,\n",
    "            'count': len(file_data),\n",
    "            'accuracy': file_accuracy,\n",
    "            'f1_score': file_f1\n",
    "        })\n",
    "        \n",
    "        print(f\"{file_name}: {len(file_data)} examples, Accuracy: {file_accuracy:.4f}, F1: {file_f1:.4f}\")\n",
    "    \n",
    "    file_df = pd.DataFrame(file_results)\n",
    "    file_df = file_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "attr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attribute-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance by attribute\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy by attribute\n",
    "attr_df.plot(x='attribute', y='accuracy', kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Accuracy by Attribute')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# F1 score by attribute\n",
    "attr_df.plot(x='attribute', y='f1_score', kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "axes[0,1].set_title('F1 Score by Attribute')\n",
    "axes[0,1].set_ylabel('F1 Score')\n",
    "axes[0,1].set_xlabel('Attribute')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "# Performance by source file (if available)\n",
    "if 'source_file' in df_clean.columns and len(file_df) > 1:\n",
    "    # Accuracy by file\n",
    "    file_df.plot(x='source_file', y='accuracy', kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('Accuracy by Source File')\n",
    "    axes[1,0].set_ylabel('Accuracy')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    \n",
    "    # F1 by file\n",
    "    file_df.plot(x='source_file', y='f1_score', kind='bar', ax=axes[1,1], color='orange')\n",
    "    axes[1,1].set_title('F1 Score by Source File')\n",
    "    axes[1,1].set_ylabel('F1 Score')\n",
    "    axes[1,1].set_xlabel('Source File')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "else:\n",
    "    # Hide unused subplots if no file data\n",
    "    axes[1,0].set_visible(False)\n",
    "    axes[1,1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total examples: {len(df)}\")\n",
    "print(f\"Clean examples: {len(df_clean)}\")\n",
    "print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Overall F1 Score: {f1:.4f}\")\n",
    "print(f\"Number of unique attributes: {len(df_clean['attribute'].unique())}\")\n",
    "print(f\"Best performing attribute: {attr_df.iloc[0]['attribute']} (Accuracy: {attr_df.iloc[0]['accuracy']:.4f})\")\n",
    "print(f\"Worst performing attribute: {attr_df.iloc[-1]['attribute']} (Accuracy: {attr_df.iloc[-1]['accuracy']:.4f})\")\n",
    "\n",
    "if 'source_file' in df_clean.columns and len(file_df) > 1:\n",
    "    print(f\"\\nNumber of source files: {len(file_df)}\")\n",
    "    print(f\"Best performing file: {file_df.iloc[0]['source_file']} (Accuracy: {file_df.iloc[0]['accuracy']:.4f})\")\n",
    "    print(f\"Worst performing file: {file_df.iloc[-1]['source_file']} (Accuracy: {file_df.iloc[-1]['accuracy']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
