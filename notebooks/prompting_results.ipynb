{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "analysis-title",
   "metadata": {},
   "source": [
    "# Prompting Results Analysis\n",
    "\n",
    "This notebook analyzes the results from attribute prompting experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Load all JSON files from a folder\n",
    "results_folder = \"../results/paligemma2\"  # Update this path\n",
    "\n",
    "# Get all JSON files in the folder\n",
    "json_files = glob.glob(os.path.join(results_folder, \"*.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "# Load each file into a separate DataFrame\n",
    "dataframes = {}\n",
    "for file_path in json_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Loading {file_name}...\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_data = json.load(f)\n",
    "\n",
    "    # Create DataFrame for this file\n",
    "    df = pd.DataFrame(file_data)\n",
    "\n",
    "    # Add source file column for reference\n",
    "    df['source_file'] = file_name\n",
    "\n",
    "    # Store DataFrame with filename as key (without .json extension)\n",
    "    file_key = os.path.splitext(file_name)[0]\n",
    "    dataframes[file_key] = df\n",
    "\n",
    "    print(f\"  -> {len(df)} examples loaded into DataFrame '{file_key}'\")\n",
    "\n",
    "print(f\"\\nCreated {len(dataframes)} separate DataFrames\")\n",
    "\n",
    "# Or iterate through them:\n",
    "for file_key, df in dataframes.items():\n",
    "    print(f\"{file_key}: {len(df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses_comprehensive(df, file_key):\n",
    "    \"\"\"Comprehensive analysis including model failures\"\"\"\n",
    "\n",
    "    # Categorize all responses\n",
    "    df['response_category'] = df['response'].apply(lambda x:\n",
    "        'true' if x.strip().lower() == 'true' else\n",
    "        'false' if x.strip().lower() == 'false' else\n",
    "        'no_prediction'\n",
    "    )\n",
    "\n",
    "    # Calculate different accuracy metrics\n",
    "    total_samples = len(df)\n",
    "\n",
    "    # Only valid predictions\n",
    "    valid_df = df[df['response_category'].isin(['true', 'false'])].copy()\n",
    "    valid_df['predicted'] = (valid_df['response_category'] == 'true').astype(int)\n",
    "\n",
    "    if len(valid_df) > 0:\n",
    "        valid_accuracy = (valid_df['predicted'] == valid_df['label']).mean()\n",
    "    else:\n",
    "        valid_accuracy = 0.0\n",
    "\n",
    "    # Conservative accuracy (treat no_prediction as wrong)\n",
    "    df['predicted_conservative'] = df.apply(lambda row:\n",
    "        1 if row['response_category'] == 'true' else\n",
    "        0 if row['response_category'] == 'false' else\n",
    "        1 - row['label']  # Opposite of true label\n",
    "    , axis=1)\n",
    "\n",
    "    conservative_accuracy = (df['predicted_conservative'] == df['label']).mean()\n",
    "\n",
    "    # Response breakdown\n",
    "    response_counts = df['response_category'].value_counts()\n",
    "\n",
    "    results = {\n",
    "        'total_samples': total_samples,\n",
    "        'valid_predictions': len(valid_df),\n",
    "        'no_predictions': len(df[df['response_category'] == 'no_prediction']),\n",
    "        'valid_accuracy': valid_accuracy,\n",
    "        'conservative_accuracy': conservative_accuracy,\n",
    "        'no_prediction_rate': len(df[df['response_category'] == 'no_prediction']) / total_samples,\n",
    "        'response_breakdown': response_counts.to_dict()\n",
    "    }\n",
    "\n",
    "    return results, valid_df\n",
    "\n",
    "# Analyze all dataframes\n",
    "analysis_results = {}\n",
    "valid_dataframes = {}\n",
    "\n",
    "for file_key, df in dataframes.items():\n",
    "    print(f\"\\n{file_key}:\")\n",
    "    results, valid_df = analyze_responses_comprehensive(df, file_key)\n",
    "\n",
    "    print(f\"  Total samples: {results['total_samples']:,}\")\n",
    "    print(f\"  Valid predictions: {results['valid_predictions']:,} ({results['valid_predictions']/results['total_samples']*100:.1f}%)\")\n",
    "    print(f\"  No predictions: {results['no_predictions']:,} ({results['no_prediction_rate']*100:.1f}%)\")\n",
    "    print(f\"  Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "    print(f\"  Conservative accuracy: {results['conservative_accuracy']:.3f}\")\n",
    "\n",
    "    analysis_results[file_key] = results\n",
    "    valid_dataframes[file_key] = valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taxonomy for color coding\n",
    "TAXONOMY_FILE = \"../dataset/mcrae-x-things-taxonomy-simp.json\"\n",
    "\n",
    "def load_taxonomy(taxonomy_file):\n",
    "    \"\"\"Load taxonomy file.\"\"\"\n",
    "    with open(taxonomy_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "taxonomy = load_taxonomy(TAXONOMY_FILE)\n",
    "print(f\"Loaded taxonomy with {len(taxonomy)} attributes\")\n",
    "print(f\"Categories: {sorted(set(taxonomy.values()))}\")\n",
    "\n",
    "# Fixed category order for presentation\n",
    "fixed_category_order = [\n",
    "    \"an_animal\",\n",
    "    \"a_bird\", \n",
    "    \"a_food\",\n",
    "    \"a_vehicle\",\n",
    "    \"beh_-_flies\",\n",
    "    \"has_legs\",\n",
    "]\n",
    "\n",
    "# Get attributes for F1 visualization with taxonomy-based color coding\n",
    "attributes_with_categories = []\n",
    "f1_scores_ordered = []\n",
    "categories_ordered = []\n",
    "\n",
    "# First, add attributes in the fixed category order\n",
    "for category in fixed_category_order:\n",
    "    category_attrs = [(attr, f1_results[attr]['f1_score']) for attr in f1_results.keys() \n",
    "                     if taxonomy.get(attr) == category]\n",
    "    # Sort attributes within category by F1 score (descending)\n",
    "    category_attrs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for attr, f1_score in category_attrs:\n",
    "        attributes_with_categories.append(attr)\n",
    "        f1_scores_ordered.append(f1_score)\n",
    "        categories_ordered.append(category)\n",
    "\n",
    "# Add any remaining attributes not in fixed order\n",
    "remaining_categories = set(taxonomy.get(attr) for attr in f1_results.keys()) - set(fixed_category_order)\n",
    "for category in sorted(remaining_categories):\n",
    "    if category:  # Skip None values\n",
    "        category_attrs = [(attr, f1_results[attr]['f1_score']) for attr in f1_results.keys() \n",
    "                         if taxonomy.get(attr) == category]\n",
    "        category_attrs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for attr, f1_score in category_attrs:\n",
    "            attributes_with_categories.append(attr)\n",
    "            f1_scores_ordered.append(f1_score)\n",
    "            categories_ordered.append(category)\n",
    "\n",
    "# Get unique categories and assign colors\n",
    "unique_categories = []\n",
    "for cat in fixed_category_order:\n",
    "    if cat in categories_ordered:\n",
    "        unique_categories.append(cat)\n",
    "\n",
    "# Add any additional categories\n",
    "for cat in categories_ordered:\n",
    "    if cat not in unique_categories:\n",
    "        unique_categories.append(cat)\n",
    "\n",
    "category_colors = dict(\n",
    "    zip(unique_categories, plt.cm.Set3(np.linspace(0, 1, len(unique_categories))))\n",
    ")\n",
    "\n",
    "# Create single F1 score plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "x_pos = range(len(attributes_with_categories))\n",
    "bar_colors = [category_colors[cat] for cat in categories_ordered]\n",
    "\n",
    "# Create bars\n",
    "bars = plt.bar(\n",
    "    x_pos,\n",
    "    [score * 100 for score in f1_scores_ordered],  # Convert to percentage\n",
    "    color=bar_colors,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for i, (bar, score) in enumerate(zip(bars, f1_scores_ordered)):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + max(f1_scores_ordered) * 100 * 0.01,\n",
    "        f\"{score * 100:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Add category separators and labels\n",
    "current_cat = None\n",
    "cat_positions = {}\n",
    "\n",
    "for i, cat in enumerate(categories_ordered):\n",
    "    if cat not in cat_positions:\n",
    "        cat_positions[cat] = []\n",
    "    cat_positions[cat].append(i)\n",
    "\n",
    "    if current_cat is not None and cat != current_cat:\n",
    "        plt.axvline(x=i - 0.5, color=\"gray\", linestyle=\"-\", linewidth=2, alpha=0.6)\n",
    "    current_cat = cat\n",
    "\n",
    "# Add category labels at the top\n",
    "y_max = max(f1_scores_ordered) * 100 * 1.15\n",
    "for cat, positions in cat_positions.items():\n",
    "    center_pos = (min(positions) + max(positions)) / 2\n",
    "    plt.text(\n",
    "        center_pos,\n",
    "        y_max * 0.92,\n",
    "        cat,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round,pad=0.3\",\n",
    "            facecolor=category_colors[cat],\n",
    "            alpha=0.3,\n",
    "            edgecolor=\"black\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Attributes (grouped by category)\", fontsize=12)\n",
    "plt.ylabel(\"F1 Score (%)\", fontsize=12)\n",
    "plt.title(\"F1 Scores by Attribute - Prompting Results\", fontsize=20)\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Set x-axis labels\n",
    "plt.xticks(x_pos, attributes_with_categories, rotation=45, ha=\"right\", fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 110)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nF1 Score Summary:\")\n",
    "print(f\"Mean F1: {np.mean(f1_scores_ordered):.3f}\")\n",
    "print(f\"Std F1:  {np.std(f1_scores_ordered):.3f}\")\n",
    "print(f\"Min F1:  {np.min(f1_scores_ordered):.3f} ({attributes_with_categories[np.argmin(f1_scores_ordered)]})\")\n",
    "print(f\"Max F1:  {np.max(f1_scores_ordered):.3f} ({attributes_with_categories[np.argmax(f1_scores_ordered)]})\")\n",
    "\n",
    "# Print breakdown by category\n",
    "print(f\"\\nF1 Score by Category:\")\n",
    "print(\"-\" * 50)\n",
    "for cat in unique_categories:\n",
    "    cat_scores = [f1_scores_ordered[i] for i, c in enumerate(categories_ordered) if c == cat]\n",
    "    if cat_scores:\n",
    "        print(f\"{cat:<20}: Mean={np.mean(cat_scores):.3f}, Min={np.min(cat_scores):.3f}, Max={np.max(cat_scores):.3f}, n={len(cat_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compmech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
