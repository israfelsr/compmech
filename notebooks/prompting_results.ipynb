{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "analysis-title",
   "metadata": {},
   "source": [
    "# Prompting Results Analysis\n",
    "\n",
    "This notebook analyzes the results from attribute prompting experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Load all JSON files from a folder\n",
    "results_folder = \"../results/paligemma2\"  # Update this path\n",
    "\n",
    "# Get all JSON files in the folder\n",
    "json_files = glob.glob(os.path.join(results_folder, \"*.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "# Load each file into a separate DataFrame\n",
    "dataframes = {}\n",
    "for file_path in json_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Loading {file_name}...\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_data = json.load(f)\n",
    "\n",
    "    # Create DataFrame for this file\n",
    "    df = pd.DataFrame(file_data)\n",
    "\n",
    "    # Add source file column for reference\n",
    "    df['source_file'] = file_name\n",
    "\n",
    "    # Store DataFrame with filename as key (without .json extension)\n",
    "    file_key = os.path.splitext(file_name)[0]\n",
    "    dataframes[file_key] = df\n",
    "\n",
    "    print(f\"  -> {len(df)} examples loaded into DataFrame '{file_key}'\")\n",
    "\n",
    "print(f\"\\nCreated {len(dataframes)} separate DataFrames\")\n",
    "\n",
    "# Or iterate through them:\n",
    "for file_key, df in dataframes.items():\n",
    "    print(f\"{file_key}: {len(df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses_comprehensive(df, file_key):\n",
    "    \"\"\"Comprehensive analysis including model failures\"\"\"\n",
    "\n",
    "    # Categorize all responses\n",
    "    df['response_category'] = df['response'].apply(lambda x:\n",
    "        'true' if x.strip().lower() == 'true' else\n",
    "        'false' if x.strip().lower() == 'false' else\n",
    "        'no_prediction'\n",
    "    )\n",
    "\n",
    "    # Calculate different accuracy metrics\n",
    "    total_samples = len(df)\n",
    "\n",
    "    # Only valid predictions\n",
    "    valid_df = df[df['response_category'].isin(['true', 'false'])].copy()\n",
    "    valid_df['predicted'] = (valid_df['response_category'] == 'true').astype(int)\n",
    "\n",
    "    if len(valid_df) > 0:\n",
    "        valid_accuracy = (valid_df['predicted'] == valid_df['label']).mean()\n",
    "    else:\n",
    "        valid_accuracy = 0.0\n",
    "\n",
    "    # Conservative accuracy (treat no_prediction as wrong)\n",
    "    df['predicted_conservative'] = df.apply(lambda row:\n",
    "        1 if row['response_category'] == 'true' else\n",
    "        0 if row['response_category'] == 'false' else\n",
    "        1 - row['label']  # Opposite of true label\n",
    "    , axis=1)\n",
    "\n",
    "    conservative_accuracy = (df['predicted_conservative'] == df['label']).mean()\n",
    "\n",
    "    # Response breakdown\n",
    "    response_counts = df['response_category'].value_counts()\n",
    "\n",
    "    results = {\n",
    "        'total_samples': total_samples,\n",
    "        'valid_predictions': len(valid_df),\n",
    "        'no_predictions': len(df[df['response_category'] == 'no_prediction']),\n",
    "        'valid_accuracy': valid_accuracy,\n",
    "        'conservative_accuracy': conservative_accuracy,\n",
    "        'no_prediction_rate': len(df[df['response_category'] == 'no_prediction']) / total_samples,\n",
    "        'response_breakdown': response_counts.to_dict()\n",
    "    }\n",
    "\n",
    "    return results, valid_df\n",
    "\n",
    "# Analyze all dataframes\n",
    "analysis_results = {}\n",
    "valid_dataframes = {}\n",
    "\n",
    "for file_key, df in dataframes.items():\n",
    "    print(f\"\\n{file_key}:\")\n",
    "    results, valid_df = analyze_responses_comprehensive(df, file_key)\n",
    "\n",
    "    print(f\"  Total samples: {results['total_samples']:,}\")\n",
    "    print(f\"  Valid predictions: {results['valid_predictions']:,} ({results['valid_predictions']/results['total_samples']*100:.1f}%)\")\n",
    "    print(f\"  No predictions: {results['no_predictions']:,} ({results['no_prediction_rate']*100:.1f}%)\")\n",
    "    print(f\"  Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "    print(f\"  Conservative accuracy: {results['conservative_accuracy']:.3f}\")\n",
    "\n",
    "    analysis_results[file_key] = results\n",
    "    valid_dataframes[file_key] = valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create F1 score visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Extract data for plotting\n",
    "attributes = list(f1_results.keys())\n",
    "f1_scores = [f1_results[attr]['f1_score'] for attr in attributes]\n",
    "accuracies = [f1_results[attr]['accuracy'] for attr in attributes]\n",
    "\n",
    "# Plot 1: F1 Scores\n",
    "bars1 = ax1.bar(attributes, f1_scores, color='skyblue', alpha=0.7)\n",
    "ax1.set_title('F1 Scores by Attribute', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('F1 Score', fontsize=12)\n",
    "ax1.set_xlabel('Attribute', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars1, f1_scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: F1 vs Accuracy comparison\n",
    "bars2 = ax2.bar(range(len(attributes)), f1_scores, width=0.4, label='F1 Score', \n",
    "                color='skyblue', alpha=0.7)\n",
    "bars3 = ax2.bar([x + 0.4 for x in range(len(attributes))], accuracies, width=0.4, \n",
    "                label='Accuracy', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax2.set_title('F1 Score vs Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_xlabel('Attribute', fontsize=12)\n",
    "ax2.set_xticks([x + 0.2 for x in range(len(attributes))])\n",
    "ax2.set_xticklabels(attributes, rotation=45)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nF1 Score Summary:\")\n",
    "print(f\"Mean F1: {np.mean(f1_scores):.3f}\")\n",
    "print(f\"Std F1:  {np.std(f1_scores):.3f}\")\n",
    "print(f\"Min F1:  {np.min(f1_scores):.3f} ({attributes[np.argmin(f1_scores)]})\")\n",
    "print(f\"Max F1:  {np.max(f1_scores):.3f} ({attributes[np.argmax(f1_scores)]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for all attributes\n",
    "f1_results = {}\n",
    "\n",
    "for file_key, valid_df in valid_dataframes.items():\n",
    "    if len(valid_df) > 0:\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(valid_df['label'], valid_df['predicted'])\n",
    "        accuracy = accuracy_score(valid_df['label'], valid_df['predicted'])\n",
    "        \n",
    "        # Clean attribute name (remove filename prefix)\n",
    "        if 'hf_inference_results_' in file_key:\n",
    "            attribute_name = file_key.replace('hf_inference_results_', '')\n",
    "        else:\n",
    "            attribute_name = file_key\n",
    "        \n",
    "        f1_results[attribute_name] = {\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'valid_samples': len(valid_df),\n",
    "            'total_samples': analysis_results[file_key]['total_samples']\n",
    "        }\n",
    "\n",
    "# Display F1 results\n",
    "print(\"F1 Scores by Attribute:\")\n",
    "print(\"-\" * 50)\n",
    "for attr, metrics in f1_results.items():\n",
    "    print(f\"{attr:<20} F1: {metrics['f1_score']:.3f}  Acc: {metrics['accuracy']:.3f}  ({metrics['valid_samples']}/{metrics['total_samples']} samples)\")\n",
    "\n",
    "f1_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compmech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
