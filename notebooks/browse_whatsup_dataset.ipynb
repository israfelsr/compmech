{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Up VLMs Dataset Browser\n",
    "\n",
    "This notebook helps browse and understand the Kamatha \"What's Up\" dataset with multiple subsets:\n",
    "- Visual Genome Q&A (one/two objects)\n",
    "- COCO Q&A (one/two objects) \n",
    "- Controlled images\n",
    "- Controlled CLEVR"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset Format\n\n**Important:** Each dataset item follows this structure:\n```\n[image_id, correct_caption, incorrect_caption, ...]\n```\n\n- **Index 0**: Image ID (or filename)\n- **Index 1**: CORRECT caption (always first)\n- **Index 2**: INCORRECT caption (always second)\n- Additional fields may exist at indices 3+\n\nThe datasets test VLM understanding of spatial relations by providing pairs of captions where only one correctly describes the spatial relationship in the image.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from collections import Counter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path on cluster\n",
    "BASE_PATH = Path(\"/leonardo_work/EUHPC_D27_102/compmech/whatsup_vlms_data/\")\n",
    "\n",
    "# Dataset configurations\n",
    "DATASETS = {\n",
    "    'vg_two_obj': {\n",
    "        'json': BASE_PATH / 'vg_qa_two_obj.json',\n",
    "        'images': BASE_PATH / 'vg_images',\n",
    "        'description': 'Visual Genome - Two objects (left/right)'\n",
    "    },\n",
    "    'vg_one_obj': {\n",
    "        'json': BASE_PATH / 'vg_qa_one_obj.json',\n",
    "        'images': BASE_PATH / 'vg_images',\n",
    "        'description': 'Visual Genome - One object (left/right)'\n",
    "    },\n",
    "    'controlled_images': {\n",
    "        'json': BASE_PATH / 'controlled_images_dataset.json',\n",
    "        'images': BASE_PATH / 'controlled_images',\n",
    "        'description': 'Controlled images - Two objects (left/right/up/down)'\n",
    "    },\n",
    "    'coco_two_obj': {\n",
    "        'json': BASE_PATH / 'coco_qa_two_obj.json',\n",
    "        'images': BASE_PATH / 'val2017',\n",
    "        'description': 'COCO - Two objects (up/down/left/right)'\n",
    "    },\n",
    "    'coco_one_obj': {\n",
    "        'json': BASE_PATH / 'coco_qa_one_obj.json',\n",
    "        'images': BASE_PATH / 'val2017',\n",
    "        'description': 'COCO - One object (up/down/left/right)'\n",
    "    },\n",
    "    'controlled_clevr': {\n",
    "        'json': BASE_PATH / 'controlled_clevr_dataset.json',\n",
    "        'images': BASE_PATH / 'controlled_clevr',\n",
    "        'description': 'Controlled CLEVR (front/behind/left/right)'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_dataset(dataset_key):\n    \"\"\"Load a specific dataset\n\n    Dataset format: List of [image_id, correct_caption, incorrect_caption(s), ...]\n    - First caption option (index 1) is always CORRECT\n    - Second caption option (index 2) is always INCORRECT\n    \"\"\"\n    config = DATASETS[dataset_key]\n    with open(config['json'], 'r') as f:\n        data = json.load(f)\n    return data, config\n\ndef get_image_path(config, image_id, dataset_key):\n    \"\"\"Get full path to image based on dataset conventions\"\"\"\n    if 'coco' in dataset_key:\n        # COCO uses zero-padded 12-digit IDs\n        filename = f\"{str(image_id).zfill(12)}.jpg\"\n    else:\n        # Other datasets may use different conventions\n        filename = f\"{image_id}.jpg\" if not str(image_id).endswith('.jpg') else image_id\n    return config['images'] / filename"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_dataset(data, dataset_key):\n    \"\"\"Analyze and print dataset statistics\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Dataset: {DATASETS[dataset_key]['description']}\")\n    print(f\"{'='*60}\")\n    \n    # Total samples\n    print(f\"\\nTotal samples: {len(data)}\")\n    \n    # Data format: [image_id, correct_caption, incorrect_caption, ...]\n    if data:\n        print(f\"\\nData format: [image_id, correct_caption, incorrect_caption, ...]\")\n        print(f\"Number of fields per item: {len(data[0])}\")\n        print(f\"\\nFirst example:\")\n        example = data[0]\n        print(f\"  Image ID: {example[0]}\")\n        print(f\"  Correct caption: {example[1]}\")\n        print(f\"  Incorrect caption: {example[2]}\")\n        if len(example) > 3:\n            print(f\"  Additional fields: {example[3:]}\")\n    \n    # Extract correct captions to analyze prepositions\n    correct_captions = [item[1] for item in data]\n    \n    # Extract spatial relations (prepositions)\n    spatial_words = ['left', 'right', 'above', 'below', 'up', 'down', 'front', 'behind']\n    caption_relations = []\n    \n    for caption in correct_captions:\n        caption_lower = caption.lower()\n        for word in spatial_words:\n            if word in caption_lower:\n                caption_relations.append(word)\n                break\n    \n    if caption_relations:\n        relation_dist = Counter(caption_relations)\n        print(f\"\\nSpatial relation distribution:\")\n        for rel, count in relation_dist.most_common():\n            print(f\"  {rel}: {count} ({count/len(data)*100:.1f}%)\")\n    \n    # Show some example captions\n    print(f\"\\nExample correct captions:\")\n    for caption in correct_captions[:5]:\n        print(f\"  - {caption}\")\n    \n    return data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all datasets\n",
    "for dataset_key in DATASETS.keys():\n",
    "    try:\n",
    "        data, config = load_dataset(dataset_key)\n",
    "        analyze_dataset(data, dataset_key)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading {dataset_key}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def display_example(data, config, index, dataset_key):\n    \"\"\"Display a single example with image and captions\n    \n    Data format: [image_id, correct_caption, incorrect_caption, ...]\n    \"\"\"\n    example = data[index]\n    \n    image_id = example[0]\n    correct_caption = example[1]\n    incorrect_caption = example[2]\n    \n    # Create figure with image and text\n    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n    \n    # Load and display image\n    img_path = get_image_path(config, image_id, dataset_key)\n    \n    if img_path.exists():\n        img = Image.open(img_path)\n        ax.imshow(img)\n        ax.axis('off')\n    else:\n        ax.text(0.5, 0.5, f\"Image not found:\\n{img_path}\", \n                ha='center', va='center', fontsize=10)\n        ax.axis('off')\n    \n    # Display captions as title with color coding\n    title = f\"✓ CORRECT: {correct_caption}\\n✗ INCORRECT: {incorrect_caption}\"\n    plt.title(title, fontsize=12, pad=20, loc='left')\n    \n    # Print all metadata\n    print(f\"\\nExample {index + 1}/{len(data)}\")\n    print(\"-\" * 70)\n    print(f\"Image ID: {image_id}\")\n    print(f\"Image path: {img_path}\")\n    print(f\"\\n✓ CORRECT caption:   {correct_caption}\")\n    print(f\"✗ INCORRECT caption: {incorrect_caption}\")\n    \n    if len(example) > 3:\n        print(f\"\\nAdditional fields: {example[3:]}\")\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive browser\ndef create_browser(dataset_key):\n    \"\"\"Create an interactive browser for a dataset\"\"\"\n    data, config = load_dataset(dataset_key)\n    \n    # Widgets\n    index_slider = widgets.IntSlider(\n        value=0,\n        min=0,\n        max=len(data)-1,\n        step=1,\n        description='Index:',\n        continuous_update=False,\n        style={'description_width': 'initial'}\n    )\n    \n    random_button = widgets.Button(description=\"Random Example\")\n    \n    # Filter by spatial relation\n    correct_captions = [item[1] for item in data]\n    spatial_words = ['left', 'right', 'above', 'below', 'up', 'down', 'front', 'behind']\n    relations_found = set()\n    for caption in correct_captions:\n        for word in spatial_words:\n            if word in caption.lower():\n                relations_found.add(word)\n    \n    filter_dropdown = widgets.Dropdown(\n        options=['all'] + sorted(list(relations_found)),\n        value='all',\n        description='Filter by:',\n        style={'description_width': 'initial'}\n    )\n    \n    output = widgets.Output()\n    \n    # Store filtered indices\n    filtered_indices = list(range(len(data)))\n    \n    def update_filtered_indices():\n        nonlocal filtered_indices\n        if filter_dropdown.value == 'all':\n            filtered_indices = list(range(len(data)))\n        else:\n            filtered_indices = [i for i, item in enumerate(data) \n                              if filter_dropdown.value in item[1].lower()]\n        index_slider.max = max(0, len(filtered_indices) - 1)\n        index_slider.value = 0\n    \n    def on_index_change(change):\n        with output:\n            clear_output(wait=True)\n            if filtered_indices:\n                actual_index = filtered_indices[change['new']]\n                display_example(data, config, actual_index, dataset_key)\n    \n    def on_random_click(b):\n        if filtered_indices:\n            index_slider.value = random.randint(0, len(filtered_indices)-1)\n    \n    def on_filter_change(change):\n        update_filtered_indices()\n        with output:\n            clear_output(wait=True)\n            if filtered_indices:\n                display_example(data, config, filtered_indices[0], dataset_key)\n    \n    index_slider.observe(on_index_change, names='value')\n    random_button.on_click(on_random_click)\n    filter_dropdown.observe(on_filter_change, names='value')\n    \n    # Initial display\n    with output:\n        display_example(data, config, 0, dataset_key)\n    \n    controls = widgets.HBox([index_slider, filter_dropdown, random_button])\n    display(widgets.VBox([controls, output]))\n\n# Example: Browse a specific dataset\nprint(\"Available datasets:\")\nfor key, config in DATASETS.items():\n    print(f\"  - {key}: {config['description']}\")\n\nprint(\"\\nUse: create_browser('dataset_key') to browse\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse Visual Genome two objects\n",
    "create_browser('vg_two_obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse COCO two objects\n",
    "create_browser('coco_two_obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse Controlled images\n",
    "create_browser('controlled_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse Controlled CLEVR\n",
    "create_browser('controlled_clevr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_datasets():\n    \"\"\"Compare statistics across all datasets\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    axes = axes.flatten()\n    \n    for idx, (dataset_key, config) in enumerate(DATASETS.items()):\n        try:\n            data, _ = load_dataset(dataset_key)\n            \n            # Extract spatial relations from correct captions\n            correct_captions = [item[1] for item in data]\n            spatial_words = ['left', 'right', 'above', 'below', 'up', 'down', 'front', 'behind']\n            caption_relations = []\n            \n            for caption in correct_captions:\n                caption_lower = caption.lower()\n                for word in spatial_words:\n                    if word in caption_lower:\n                        caption_relations.append(word)\n                        break\n            \n            if caption_relations:\n                relation_dist = Counter(caption_relations)\n                labels = list(relation_dist.keys())\n                values = list(relation_dist.values())\n                \n                axes[idx].bar(labels, values, color='steelblue')\n                axes[idx].set_title(f\"{dataset_key}\\n({len(data)} samples)\", fontsize=11, fontweight='bold')\n                axes[idx].set_ylabel('Count', fontsize=10)\n                axes[idx].tick_params(axis='x', rotation=45, labelsize=9)\n                axes[idx].grid(axis='y', alpha=0.3)\n        except Exception as e:\n            axes[idx].text(0.5, 0.5, f\"Error: {str(e)[:50]}\", \n                          ha='center', va='center', transform=axes[idx].transAxes,\n                          fontsize=9, wrap=True)\n            axes[idx].set_title(f\"{dataset_key}\\n(Error)\", fontsize=11)\n    \n    plt.suptitle('Spatial Relation Distribution Across Datasets', fontsize=14, fontweight='bold', y=1.00)\n    plt.tight_layout()\n    plt.show()\n\ncompare_datasets()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def search_examples(dataset_key, spatial_relation=None, caption_contains=None):\n    \"\"\"Search for specific examples\n    \n    Args:\n        dataset_key: Which dataset to search\n        spatial_relation: Filter by spatial relation (e.g., 'left', 'right', 'above')\n        caption_contains: Filter by text in correct caption\n    \"\"\"\n    data, config = load_dataset(dataset_key)\n    \n    filtered = data\n    \n    if spatial_relation:\n        filtered = [item for item in filtered \n                   if spatial_relation.lower() in item[1].lower()]\n    \n    if caption_contains:\n        filtered = [item for item in filtered \n                   if caption_contains.lower() in item[1].lower()]\n    \n    print(f\"Found {len(filtered)} examples matching criteria\")\n    \n    # Show first few examples\n    if filtered:\n        print(\"\\nFirst 5 matches:\")\n        for i, item in enumerate(filtered[:5]):\n            print(f\"\\n{i+1}. Image ID: {item[0]}\")\n            print(f\"   Correct: {item[1]}\")\n            print(f\"   Incorrect: {item[2]}\")\n    \n    return filtered, config\n\n# Example usage (uncomment to use):\n# filtered_data, config = search_examples('coco_two_obj', spatial_relation='left')\n# if filtered_data:\n#     display_example(filtered_data, config, 0, 'coco_two_obj')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}