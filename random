
    dataset: Dataset,
    layers: Union[str, int, List[Union[str, int]]] = "last",
    batch_size: int = 32,
    features_base_dir: str = "external_data/cached_features"
) -> Dataset:
    """
    Add feature columns to a HuggingFace dataset.
    Add feature columns to a HuggingFace dataset by loading from separate feature datasets.
    Features are saved as independent datasets in model/layer.hf format to avoid heavy main datasets
    Args:
        dataset: Input dataset with 'image_path' column
        layers: Layer(s) to extract features from
        batch_size: Processing batch size
        features_base_dir: Base directory to save feature dataset
    Returns:
        Dataset with added feature columns
        Dataset merged with selected layer features
    """
    # Ensure layers is a list
    if not isinstance(layers, list):
        layers = [layers
    logging.info(
        f"Extracting features from layers {layers} for {len(dataset)} samples..."
        f"Processing features from layers {layers} for {len(dataset)} samples..."
    
    # Start with original dataset
    merged_dataset = dataset

    # Process each layer
    for layer in layers:
        feature_column_name = f"features_{layer}"

        # Check if column already exists
        if feature_column_name in dataset.column_names:
            logging.warning(
                f"Feature column '{feature_column_name}' already exists, skipping..."
        # Create feature dataset path: model/layer.hf
        model_name = getattr(self, 'model_name', 'unknown_model')
        safe_model_name = model_name.replace('/', '_').replace('\\', '_')
        feature_dataset_path = Path(features_base_dir) / f"{safe_model_name}_{layer}.hf"

        # Check if feature dataset already exists
        if feature_dataset_path.exists():
            logging.info(f"Loading existing feature dataset from {feature_dataset_path}")
            try:
                feature_dataset = Dataset.load_from_disk(str(feature_dataset_path))
                logging.info(f"Loaded {len(feature_dataset)} feature samples from cache")
            except Exception as e:
                logging.warning(f"Failed to load cached features: {e}. Recomputing...")
                feature_dataset = self._extract_and_save_features(
                    dataset, layer, feature_dataset_path, batch_size
                )
        else:
            logging.info(f"Feature dataset not found. Extracting features for layer '{layer}'...")
            feature_dataset = self._extract_and_save_features(
                dataset, layer, feature_dataset_path, batch_size
            )
            continue

        logging.info(f"Processing layer '{layer}'...")

        # Extract features for all samples
        all_features = []

        for i in tqdm(range(0, len(dataset), batch_size), desc=f"Layer {layer}"):
            batch_end = min(i + batch_size, len(dataset))
            batch = dataset.select(range(i, batch_end))

            batch_features = []
            for sample in batch:
                features = self.extract_features(sample["image_path"], layer)
                batch_features.append(features)

            all_features.extend(batch_features)
    
        # Add column to dataset
        dataset = dataset.add_column(feature_column_name, all_features)
    
        feature_shape = all_features[0].shape
        logging.info(
            f"Added column '{feature_column_name}' with shape {feature_shape}"

        # Merge with original dataset
        merged_dataset = self._merge_features_with_dataset(
            merged_dataset, feature_dataset, layer
        
    return dataset
    return merged_dataset

def _extract_and_save_features(
    self,
    dataset: Dataset,
    layer: Union[str, int],
    save_path: Path,
    batch_size: int
) -> Dataset:
    """
    Extract features and save as a separate dataset.
    
    Args:
        dataset: Original dataset
        layer: Layer to extract from
        save_path: Path to save the feature dataset
        batch_size: Processing batch size
    
    Returns:
        Feature dataset with columns: image_path, features
    """
    logging.info(f"Extracting features for layer '{layer}'...")
    
    # Create save directory
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Extract features for all samples
    image_paths = []
    all_features = []
    
    for i in tqdm(range(0, len(dataset), batch_size), desc=f"Layer {layer}"):
        batch_end = min(i + batch_size, len(dataset))
        batch = dataset.select(range(i, batch_end))
    
        batch_features = []
        batch_paths = []
    
        for sample in batch:
            features = self._extract_single_image_features(sample["image_path"], layer)
            batch_features.append(features)
            batch_paths.append(sample["image_path"])
    
        all_features.extend(batch_features)
        image_paths.extend(batch_paths)
    
    # Create feature dataset with just image_path and features
    feature_data = {
        'image_path': image_paths,
        'features': all_features
    }
    
    feature_dataset = Dataset.from_dict(feature_data)
    
    # Save the feature dataset
    feature_dataset.save_to_disk(str(save_path))
    
    feature_shape = all_features[0].shape if all_features else "unknown"
    logging.info(f"Saved feature dataset to {save_path} with feature shape {feature_shape}")
    
    return feature_dataset
    
def _extract_single_image_features(self, image_path: str, layer: Union[str, int]) -> np.ndarray:
    """
    Extract features from a single image.
    
    Args:
        image_path: Path to image file
        layer: Layer to extract from
    
    Returns:
        Feature vector as numpy array
    """
    try:
        # Load and preprocess image
        image = Image.open(image_path).convert('RGB')
        inputs = self.processor(images=image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
    
        # Extract features
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
    
            if layer == "last":
                features = outputs.last_hidden_state[:, 0]  # CLS token
            elif layer == "intermediate":
                # Average of intermediate layers
                hidden_states = outputs.hidden_states
                features = torch.stack(hidden_states).mean(dim=0)[:, 0]
            elif isinstance(layer, int):
                # Specific layer
                features = outputs.hidden_states[layer][:, 0]
            else:
                raise ValueError(f"Unknown layer specification: {layer}")
    
        return features.cpu().numpy().flatten()
    
    except Exception as e:
        logging.error(f"Error extracting features from {image_path}: {e}")
        # Return zero vector as fallback
        feature_dim = getattr(self, 'feature_dim', 768)  # Default for DINOv2 base
        return np.zeros(feature_dim)